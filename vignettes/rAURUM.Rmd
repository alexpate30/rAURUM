---
title: "user-guide"
output: 
  rmarkdown::html_vignette:
    number_sections: true
vignette: >
  %\VignetteIndexEntry{user-guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

*This article does not contain any real patient data. All patient data has been simulated but formatted to match the structure of CPRD Aurum data.*

# Introduction

Clinical Practice Research Datalink (CPRD) is a primary care database containing information on demography, medical history, test results and drug use. CPRD maintain two databases, CPRD GOLD which contains data from general practices using the Vision computer system, and CPRD Aurum, which contains data from general practices using the EMIS computer system. As of June 2024, these databases contain data on 21 million (3 million currently registered) https://www.cprd.com/doi/cprd-gold-june-2024-dataset and 47 million (16 million currently registered) https://www.cprd.com/doi/cprd-aurum-march-2024-dataset#:~:text=CPRD%20Aurum%20database%20March%202024,%3A%209.55%20(3.30%20%E2%80%93%2022.22) individuals respectively. CPRD is a widely used resource. Since 2019, using the PubMed library, there has been 540 studies published which contain "CPRD" in the title or abstract. Extraction of CPRD data into an analysable format is a computationally demanding process and requires a significant amount of work. Despite this, we are unaware of any software available to aid researchers in the extraction and processing of CPRD data except rEHR (Springate et al., 2017) XXXX, which is no longer maintained and has been removed from CRAN. 

This study introduces **rAURUM**, an R package designed to assist researchers in working with CPRD Aurum data and creating datasets which are analysis-ready. We start by outlining the issues encountered when working with CPRD Aurum data and the approach taken by **rAURUM** for processing this data, which draws heavily on the work of Springate et al (2017) XXXX. We then run through a worked example to showcase the functionality of rAURUM. We finish with a discussion. We focus on CPRD Aurum, as opposed to CPRD GOLD, given there has been a considerable drop in the number of practices utilising Vision software in the last 10 years, limiting the research utility of CPRD GOLD database. This package can also be used to manage linked secondary care (HES) and ONS death data.

# Data Structure and Extraction Process

## Structure of CPRD Aurum data

We first define some terminology which will be used throughout this article. *Raw data*: The raw data provided to the user by CPRD. *Cohort:* A cohort of individuals that meet the inclusion/exclusion criteria for a given research question. In this setting, the cohort is ultimately a vector of patient id's. *Analysis-ready dataset*: A data frame with one row for each individual in the cohort, and a column for each variable of interest, for example, age at cohort entry, or most recent BMI score prior to cohort entry.

The raw data is split into eight different file types: Consultation, DrugIssue, Observation, Patient, Practice, Problem, Referral, Staff. The data specification is available here: https://cprd.com/sites/default/files/2022-02/CPRD%20Aurum%20Data%20Specification%20v2.7%20%28002%29.pdf. REF XXXX. For most research questions, the relevant files are Patient, Observation and DrugIssue. The Patient file contains information about registration into the database, date of death or lost to follow up, year of birth and gender. This file will be  required to define a cohort. The observation file contains all medical diagnoses and tests, while drugissue contains information on prescriptions. Medical observations are identified by their *medcodeid*, whereas prescriptions are identified through their *prodcodeid*.

In order to facilitate data transfer, this data is commonly split into into numerous smaller files. The different patient files are denoted by the string *set1*, *set2*, *set3* in the file name. Individuals in the same patient file will have the corresponding string (*setX*) in the files containing their medical or prescription data. However, there will be more than one Observation and DrugIssue file corresponding to each patient file. For example, the observation files for patients in *set1*, will have *set1* in their file name, and then an extra suffix 1, 2, 3, etc. The same is true for the DrugIssue files. The naming structure for these is as follows:

- aurum_allpatid_set*X*_extract_patient_001.txt
- aurum_allpatid_set*X*_extract_observation_00*Y*.txt
- aurum_allpatid_set*X*_extract_drugissue_00*Y*.txt

where *X* and *Y* take integer values. Note that prefix to the file names may vary (i.e. the *aurum_allpatid* part) however we expect the naming convention with regards to *setX*, file type, and *00Y* to remain consistent. If this changes in the future, we will endeavour to update the rAURUM as soon as possible. 

## Overarching process

The main problem when working with CPRD Aurum data is the size of the raw data. Data on over 47 million individuals results in thousands of raw .txt files, and Terabytes of data, which can be cumbersome work with. This is a particular issue for R users, as its unfeasible to read all this data into the R workspace simultaneously, as R operates using RAM. As suggested by Springate et al., (2017 REF XXXX) **rAURUM** bypassess this problem by creating an SQLite database which can be saved onto the hard disk on your computer system. This SQLite database can then be queried for data of interest in order to build an analysis-ready dataset.

Our recommended process for developing an analysis-ready dataset is as follows (see Figure XXXX for a visual representation of this):

* Step 1: Define cohort of interest by applying inclusion/exclusion criteria.
* Step 2: Read raw data for individuals in the cohort of interest into R, and write this data into an SQLite database.
* Step 3: Query this SQLite database for specific codes and tests to create variables for each individual in the cohort.
* Step 4: Combine extracted variables into an analysis-ready dataset.

We recommend this process because once set up, querying the SQLite database is computationally much quicker than reading each of the raw files into the R workspace and querying these separately. It also reduces the probability of errors induced from creating numerous for loops when looping through the raw data files. We now move onto a worked example, where we showcase how to implement the above process using **rAURUM**.

# Worked example for data extraction

## Step 1: Defining a cohort

We have provided simulated patient, observation and drugissue files with these naming conventions, which will be utilisied in the worked example. They are contained in the *inst/aurum_data* directory of **rAURUM**. After installing **rAURUM**, this directory can be accessed using the command `system.file("aurum_data", package = "rAURUM")`. This contains data on 12 fake patients, split across two patient files (*set1* and *set2*) and three observation and drugissue files (all *set1*):

```{r}
#devtools::install_github("alexpate30/rAURUM")
#install.packages("rAURUM") NOT YET ON CRAN
library(rAURUM)
list.files(system.file("aurum_data", package = "rAURUM"), pattern = ".txt")
```
The first step in most analyses is creating and defining a cohort of individuals, which will involve working with the patient files. Data from the patient files can be combined using the `extract_patients` function. This will look in the directory specified through the `filepath` argument, for any file containing "patient" in the file name. All files will be read in and concatenated into a single dataset. In some circumstances, researchers may be provided with a list of patids which meet their inclusion/exclusion criteria. In this case, these can be specified through the `patids` argument (which requires a character vector). Suppose the individuals meeting the exclusion criteria are those with patid = 1, 3, 4 and 6. We would then specify:

```{r}
pat <- extract_patients(filepath = system.file("aurum_data", package = "rAURUM"), patids = as.character(c(1,3,4,6)))
str(pat)
```
In other circumstances, a user may need to apply the inclusion and exclusion criteria themselves. In this case, one would initially create a patient file for all individuals.

```{r}
pat <- extract_patients(filepath = system.file("aurum_data", package = "rAURUM"))
str(pat)
```
The cohort of individuals would then be defined by applying study specific inclusion/exclusion criteria. For example, all individuals with > 1 day valid follow up aged 65+, after 1st January 2000. Such criteria can be applied solely using the information available in patient files. In this example, we define the individuals that met the inclusion criteria to be those with patid = 1, 3, 4 and 6. 

```{r}
pat <- subset(pat, patid %in% c(1,3,4,6))
```

Once the cohort has been defined, the next step is to extract medical/prescription data for these individuals. However, it is also common for inclusion/exclusion criteria to be dependent on medical diagnoses and prescription history. For example, an additional inclusion criteria that individuals must have a diagnosis of type 2 diabetes. To apply this criteria and reduce the cohort further, you must first extract medical history data for the cohort. In this case, the steps outlined in section XXXX may be altered to:

* Step 1: Define cohort of interest by applying inclusion/exclusion criteria that are not dependent on medical history.
* Step 2: Read medical/prescription data for cohort of interest into R and write into an SQLite database.
* Step 2.2: Query this SQLite database for specific codes and tests to apply remaining inclusion/exclusion criteria that are dependent on medical history.
* Step 2.3: [Optional] Reduce SQLite database to only contain data on individuals in final cohort. This may be worthwhile if the inclusion/exclusion criteria in step 3 excluded a large number of individuals. This will mean future queries into the SQLite database will run much quicker.
* Step 3: Query this SQLite database for specific codes and tests to create variables for each individual in the cohort.
* Step 4: Combine extracted variables into an analysis-ready dataset.

## Step 2: Reading in data and creating an SQLite database

Data for individuals in the cohort of interest is extracted from the .txt files and put into a SQLite database. This SQLite database is stored on the hard disk and can be queried when defining an analysis-ready dataset.

### Add individual files to SQLite database using `add_to_database`

The function `add_to_database` can be used to add individuals files to the SQLite database. Start by defining and connecting to your SQLite database. In this article we create a temporary database, but in practice this would be a permanent location the hard disk. Specifically, `tempfile("temp.sqlite")` would be replaced by the desired file path and SQLite database name.

```{r}
aurum_extract <- connect_database(tempfile("temp.sqlite"))
```

Next we add medical diagnoses data from the observation files to a table within this database, which we call `obs`, using the `add_to_database` function. The simulated raw data provided with \pkg{rAURUM} can be accessed using the `system.file` function. The vector of patient id's that defines the cohort is defined through the `subset.patids` argument. Only data with patid's matching this argument will be added to the SQLite database.The `filetype` argument will select an appropriate function for reading in the .txt files, and also defines the name of the table in the SQLite database that the files are added to. Note that for the first file, `overwrite = TRUE` is specified to create a new table. For the second and third file, `append = TRUE` is specified to append to an existing table.

```{r}
add_to_database(filepath = system.file("aurum_data", "aurum_allpatid_set1_extract_observation_001.txt", package = "rAURUM"), 
                filetype = "observation", nrows = -1, select = NULL, subset.patids = c(1,3,4,6), use.set = FALSE, db = aurum_extract, overwrite = TRUE)
add_to_database(filepath = system.file("aurum_data", "aurum_allpatid_set1_extract_observation_001.txt", package = "rAURUM"), 
                filetype = "observation", subset.patids = c(1,3,4,6), db = aurum_extract, append = TRUE)
add_to_database(filepath = system.file("aurum_data", "aurum_allpatid_set1_extract_observation_001.txt", package = "rAURUM"), 
                filetype = "observation", subset.patids = c(1,3,4,6), db = aurum_extract, append = TRUE)
```

We can then query this database, by selecting all rows from the *observation* table, and only printing the first 3. More details on how to query an SQLite database using \pkg{RSQLite} is available **HERE XXXX REF**. Note that when reading the .txt data into R, the dates are converted into date formats, with a underlying numeric value where day 0 is 01/01/1970. When saved to the SQLite database, it is the underlying numeric values which is saved, hence the dates now appearing as numeric values.

```{r}
RSQLite::dbGetQuery(aurum_extract, 'SELECT * FROM observation', n = 3)
```

Next we add the prescription data from the DrugIssue files to a table called `drugissue`.

```{r}
add_to_database(filepath = system.file("aurum_data", "aurum_allpatid_set1_extract_drugissue_001.txt", package = "rAURUM"), 
                filetype = "drugissue", nrows = -1, select = NULL, subset.patids = c(1,3,4,6), use.set = FALSE, aurum_extract, overwrite = TRUE)
add_to_database(filepath = system.file("aurum_data", "aurum_allpatid_set1_extract_drugissue_001.txt", package = "rAURUM"), 
                filetype = "drugissue", nrows = -1, select = NULL, subset.patids = c(1,3,4,6), use.set = FALSE, aurum_extract, append = TRUE)
add_to_database(filepath = system.file("aurum_data", "aurum_allpatid_set1_extract_drugissue_001.txt", package = "rAURUM"), 
                filetype = "drugissue", nrows = -1, select = NULL, subset.patids = c(1,3,4,6), use.set = FALSE, aurum_extract, append = TRUE)
```

Again we can then query this table, by selecting all rows from the *drugissue* table, and only printing the first 3.

```{r}
RSQLite::dbGetQuery(aurum_extract, 'SELECT * FROM drugissue', n = 3)
```

We can list all the tables in the SQLite database and see there are now two, named *observation* and *drugissue*. 

```{r}
RSQLite::dbListTables(aurum_extract)
```

The `add_to_database` function allows specification of `filetype = c("observation", "drugissue", "referral", "problem", "consultation", "hes_primary","death")`, each corresponding to a specific function for reading in the corresponding .txt files with correct formatting. The `"hes_primary"` options correspond to the primary diagnoses file in linked HES APC data. The `"death"` file corresponds to the death file in the linked ONS data. 

Finally, when manually adding files in this manner, it is good practice to close the connection to the SQLite database once finished.

```{r}
RSQLite::dbDisconnect(aurum_extract)
```

### Add all relevant files to SQLite database using `cprd_extract`

In practice, there will be a high number of files to add to the SQLite database and adding each one using `add_to_database` would be cumbersome. We now we repeat the extraction but using the cprd_extract function, which is a wrapper for `add_to_database`, and will add all the files in a specified directory.

Start by creating a connection to a new database (again, we use a temporary database, but this would be a permanent file on the hard disk in practice).

```{r}
aurum_extract <- connect_database(tempfile("temp.sqlite"))
```

We then use `cprd_extract` to add all the observation files into the SQLite database. The directory containing the files should be specified using `filepath`. It will only read in and add files with the text string specified in `filetype`, which takes values in `c("observation", "drugissue", "referral", "problem", "consultation")`. We then query the first three rows of this database, and note they are the same as previously.

```{r}
### Extract data
cprd_extract(aurum_extract, 
             filepath = system.file("aurum_data", package = "rAURUM"), 
             filetype = "observation", subset.patids = c(1,3,4,6), use.set = FALSE)

### Query first three rows
RSQLite::dbGetQuery(aurum_extract, 'SELECT * FROM observation', n = 3)
```

The process is then repeated for the drugissue files.

```{r}
### Extract data
cprd_extract(aurum_extract, 
             filepath = system.file("aurum_data", package = "rAURUM"), 
             filetype = "drugissue", subset.patids = c(1,3,4,6), use.set = FALSE)

### List tables
RSQLite::dbListTables(aurum_extract)

### Query first three rows
RSQLite::dbGetQuery(aurum_extract, 'SELECT * FROM drugissue', n = 3)

### Disconnect
RSQLite::dbDisconnect(aurum_extract)
```

Note that this function may run for a considerable period of time when working with the entire CPRD AURUM database, and therefore it is not recommended to run interactively. While creation of the SQLite database may be time consuming, subsequent queries will be far more efficient, so this is short term pain for a long term gain.

### Add all relevant files to SQLite database in a comptationally efficient manner using the `set` functionality.

When the number of patients in your cohort is very large (say 20,000,000, which was the case for us), the `add_to_database` function may perform very slowly. This is because it checks to see whether the patid for each observation in the file being added to the SQLite database, is contained in the vector `subset.patids` (a vector of length 20,000,000 in our case). We can utilise the structure of the CPRD AURUM data to speed up this process. If data has been provided using the *set* structure (see previous section), we know that we only need to search for patids from `subset.patids`, that are in the corresponding patient file. For example, when reading in file *aurum_allpatid_set1_extract_observation_00Y.txt* (for any *Y*), we only need to search whether paatid is in the vector of patids from `subset.patid`, that are also in *aurum_allpatid_set1_extract_patient_001.txt*, which is much smaller vector. This may seem excessive, but in our case this reduced the time that `add_to_database` took to run from hours to minutes. When running `cprd_extract` on hundreds on files, this reduced months of computation time (infeasible) to days.

To achieve this, the `subset.patids` object should be a dataframe with two required columns. The first column should be `patid`, the second should be `set`, reporting the corresponding value of set which the patient belongs to. The first step is therefore to create a patient file, which has an extra variable `set`, which indicates the number following the text string *set* in the patient file containing data for that patient. When reading in the patient files to create your cohort, this can be done using the function `extract_txt_pat`, and specifying `set = TRUE`. Note that in this example there is only one patient file, with `set = 1`, but in practice there would be numerous patient files that would need to be read in and concatenated to create your cohort.

```{r}
pat <- extract_txt_pat(system.file("aurum_data", "aurum_allpatid_set1_extract_patient_001.txt", package = "rAURUM"), set = TRUE)
pat
```

The patient file read in is the same as previously, with the addition of the `set` column. This file can be reduced to just the `patid` and `set` columsn, and used as the input to `subset.patids` when running the `add_to_database` and `cprd_extract` functions. When extracting data from observation files with *set1* in the name, it will only search for patient id's with `set == 1` in the data.frame provided to `subset.patids`.

```{r}
### Reduce patient cohort to just variables "patid" and "set", and where patid = 1, 3, 4 and 6.
pat <- subset(pat, patid %in% c(1,3,4,6))
pat

### Create connection to SQLite database
aurum_extract <- connect_database(tempfile("temp.sqlite"))

### Add observation files
cprd_extract(aurum_extract, 
             filepath = system.file("aurum_data", package = "rAURUM"), 
             filetype = "observation", nrows = -1, select = NULL, subset.patids = pat, use.set = TRUE)

### Add drugissue files
cprd_extract(aurum_extract, 
             filepath = system.file("aurum_data", package = "rAURUM"), 
             filetype = "drugissue", nrows = -1, select = NULL, subset.patids = pat, use.set = TRUE)

### Query first three rows of each table
RSQLite::dbGetQuery(aurum_extract, 'SELECT * FROM observation', n = 3)
RSQLite::dbGetQuery(aurum_extract, 'SELECT * FROM drugissue', n = 3)
```

Note that there is no difference compared to the previously extracted SQLite databases. The computational gains from applying the subsetting in this manner will not be realised in this example. We do not close the connection, as we will now move onto querying the database to extract variables for creating an analysis-ready dataset.

## Querying the SQLite dataabase to extract variables

Once the data has been extracted and stored in an SQLite database, it can now be queried to create variables of interest. 

### Low level functions

The `db_query` function will query the SQLite database for observations where medcodeid is in a specified codelist. For example, we can query the *observation* table for all codes with medcodeid of *187341000000114*.

```{r}
db.query <- db_query(db.open = aurum_extract,
                     tab ="observation",
                     codelist.vector = "187341000000114")

db.query
```

The `combine_query_boolean` function will assess whether each individual in a specified cohort has an observation in the queried data (obtained using `db_query`) within a specified time frame from the index date, returning a 0/1 vector. The `cohort` must contain a variable called `indexdt` containing the index date. This function is useful when defining 'history of' type variables, where we want to know if there is any record of a given condition prior to the index date. 

```{r}
### Add an index date to pat
pat$indexdt <- as.Date("01/01/2020", format = "%d/%m/%Y")

### Extract a history of type variable using extract_ho
combine.query.boolean <- combine_query_boolean(cohort = pat,
                                               db.query = db.query,
                                               query.type = "observation")
  
combine.query.boolean
```
The `combine_query` function will merge a cohort with the queried data (obtained using `db_query`), returning a specified number of observations (`numobs`) within a specified time frame from the index date. This is useful when extracting test data and requiring access to the values of the tests, or when specifying variables that require > 1 observation within a certain time frame (i.e. two prescriptions within a month prior to index date). For queries from the `observation` table, the query type can be specified as `"med"` or `"test"`. Inputting `query.type = "med"` will just return the date of the observations and the medcodeid.

```{r}
### Extract a history of type variable using extract_ho
combine.query <- combine_query(cohort = pat,
                               db.query = db.query,
                               query.type = "med",
                               numobs = 3)
  
combine.query
```

For `query.type = "test"`, the `value` and other relevant information will also be returned, and those with NA values removed (although this can be altered through argument `value.na.rm`).

```{r}
### Extract a history of type variable using extract_ho
combine.query <- combine_query(cohort = pat,
                               db.query = db.query,
                               query.type = "test",
                               numobs = 3)
  
combine.query
```

If the query was from the `drugissue` table, then `query.type = "drugissue"` should be specified. 

### Mid-level functions

However, it is not always necessary to work with these functions directly. We have written a number of functions which should cover the extraction of common variable types. The first is `extract_ho`, which defines a binary variable whether individual has a specified code recorded prior to index date. This can be applied to search for history of medical diagnoses or prescriptions. The second is `extract_time_until`, which defines a variable for time until the first of a record of a specified code or censoring, and an indicator for whether event was observed or censored. The third is `extract_test`, which will extract test results between a given time frame for a specified code list. Below are an example of each function. We use the same codelist (of length 1) for each function, given the data is simulated and we know this code is present in the data. Argument `return.output = TRUE` will return the outputted data frame into the R workspace. Further functionality of these functions can be found in the function documentation.

```{r}
### Read in codelist
codelist <- "187341000000114"

### Add an index date to pat
pat$fup_start <- as.Date("01/01/2020", format = "%d/%m/%Y")
pat$fup_end <- as.Date("01/01/2024", format = "%d/%m/%Y")

### Extract a history of type variable using extract_ho
ho <- extract_ho(pat, 
                 codelist.vector = codelist, 
                 indexdt = "fup_start", 
                 db.open = aurum_extract, 
                 tab = "observation",
                 return.output = TRUE)
str(ho)

### Extract a time until variable using extract_time_until
time_until <- extract_time_until(pat, 
                                 codelist.vector = codelist, 
                                 indexdt = "fup_start", 
                                 censdt = "fup_end",
                                 db.open = aurum_extract, 
                                 tab = "observation",
                                 return.output = TRUE)
str(time_until)

### Extract test data using extract_test_data
test_data <- extract_test_data(pat, 
                               codelist.vector = codelist, 
                               indexdt = "fup_start", 
                               db.open = aurum_extract,
                               time.prev = Inf,
                               return.output = TRUE)
str(test_data)

### Disconnect
RSQLite::dbDisconnect(aurum_extract)
```

### High level functions


Note that all the functions for extracting variables also have an option `out.save.disk` for saving the extracted data directly onto the hard disk. The filepath where to save can be specified manually through the `out.filepath` argument. However, is this argument is left as `NULL`, while `out.disk.save = TRUE`, rAURUM will automatically try to save the extracted variable into a directory "data/extraction/". The name of the file itself will be dependent on the variable name specified through argument `varname`. This can be a very convenient way to save the output directly to disk without having to repeatdly specify filepaths and filenames.

There is similar functionality when specifying the code lists. Codelists can be specified in two ways. The first is to read the codelist into R as a character vector and then specify through the argument `codelist.vector`. Codelists stored on the hard disk can also be referred to from the `codelist` argument, but require a specific underlying directory structure. The codelist on the hard disk must be stored in a directory called "codelists/analysis/" relative to the working directory. The codelist must be a .csv file, and contain a column "medcodeid", "prodcodeid" or "ICD10" depending on the input for argument `tab`. The input to argument `codelist` should just be a character string of the name of the files (excluding the suffix '.csv'). The `codelist.vector` option will take precedence over the `codelist` argument if both are specified. 

Finally, there is similar functionality for accessing the SQLite databases from within these functions, rather than having to open a connection, use this as an input in the function, and then remember to close the connection. Instead, if the SQLite database is stored in a directory "data/sql/" relative to the working directory, the SQLite database can be referred to by name (a character string) in the argument `db`. Alternatively, a SQLite database stored anywhere on the hard disk can be accessed by specifying the full filepath (character string) in the argument `db.filepath`. A connection to the SQLite datbase will be opened, the SQLite database will be queried, and then the connection closed.


### Utilising underlying directory system

**I will be writing up more documentation on the appropriate directory structures to use these functions most efficiently. It is tricky to elicit this functionality in this vignette given the required directory structures. Something I am thinking about.**




### Creation of SQLite database

Once all the variables of interest have been extracted, they can be merged into an analysis-ready dataset.

```{r}
### Recursive merge
analysis.ready.pat <- Reduce(function(df1, df2) merge(df1, df2, by = "patid", all.x = TRUE), list(pat[,c("patid", "gender", "yob")], ho, time_until, test_data)) 
analysis.ready.pat
```

# Future proofing and extension to other databases



# Functions for specific variables

Finally, a number of functions have been written for extracting specified variables, which do not fit into the above categories. See:

* `extract_age`: Derives age relative to an index date. Assumes individual born on 1st July.
* `extract_bmi`: Derives BMI scores. Requires specification of codelist for BMI, height, and weight separately.
* `extract_cholhdl_ratio`: Derives total cholesterol/high-density lipoprotein ratio. Requires specification of separate codelists for total cholesterol/high-density lipoprotein ratio, total cholesterol, and high-density lipoproteins separately.
* `extract_diabetes`: Derives a categorical variable for history of type 1 diabetes, history of type 2 diabetes or no history of diabetes. Requires specification of separate codelists for type 1 and type 2 diabetes.
* `extract_ethnicity`: Derives a categorical variable for ethnicity, categories bangladeshi, black african, black caribbean, chinese, indian, other asian, other ethnic group, pakistani or white (a separate code list is required for each).
* `extract_smoking`: Derives a categorical variable for smoking status. Requires specification of seperate codelists for non-smoker, ex-smoker, light smoker, moderate smoker and heavy smoker. If the most recent smoking status is non-smoker, but there are historical codes which indicate smoking, then individual will be classified as an ex-smoker. 
* `extract_test_data_var`: Derives standard deviation of test data values recorded over a specified time period.

# OLD TEXT I'M SAVING FOR NOW



Codes that represent the same thing (with respect to some definition of the user) are commonly combined into code lists.

We will showcase two approaches to extracting data for a cohort, one which is reliant on the data structure and naming conventions to reduce computational time, and one which has no reliance on these things.





Note that in some instances CPRD will apply the inclusion/exclusion criteria and provide a list of patid's to the user. In this case, step 1 can be ignored and the first step will be to create the SQLite database.






This article will showcase how to extract CPRD AURUM data for a cohort of individuals using the \pkg{rAURUM} package. There are two main scenarios:

- Basic, all file directories are specified manually, etc
- In-built. This requires file systems and directories to have a specific set up. However, once the data is in this format, running the extract should be much simpler.

It will then showcase some basic functions for extracting variables for this cohort, and how to build an analysis-ready dataset.

This package tackles a key problem, of working with the large CPRD AURUM datasets in R. In the June 2021 extract, the raw files amount to **XXXX** TB of data in .txt files. It is therefore unfeasible to read all these files simultaneously into the R workspace, and create a cohort in a straightforward way. We take the approach of Springate et al., **REF XXXX https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5323003/** by 





## CPRD AURUM Data structure

This vignette assumes you are working with the 'flatfiles', which is an extract of the entire CPRD AURUM database. For many research projects, you may be provided only with data for individuals that meet a certain inclusion criteria (i.e. a diagnoses of type 1 diabetes between 2000 - 2020). We believe the data is formatted the same way regardless of whether you are working with the flatfiles or not, so this vignette should still be relevant. **Would be great to get someone who is working with a dataset defined this way to test the package**. The CPRD AURUM data is split into eight different file types: Consultation, DrugIssue, Observation, Patient, Practice, Problem, Referral, Staff. The data specification is available here: https://cprd.com/sites/default/files/2022-02/CPRD%20Aurum%20Data%20Specification%20v2.7%20%28002%29.pdf. For most research questions, the relevant files are Patient, Observation and DrugIssue. The Patient file is often required to define the cohort. Observation contains all medical diagnoses and tests, while DrugIssue contains all information on prescriptions. There is a limit on file size in the raw data, and so each of the above file types is commonly split up into numerous individual files.

The patient files are split up into groups of **CHECK THIS XXXX** 1,000,0000 individuals, denoted by set1, set2, set3, etc. There will be more than 1 Observation and DrugIssue file for each set of patients, because the file sizes are capped at 1GB. The observation files for patients in *setX*, will have the corresponding set number in the file name, and then a suffix 1, 2, 3, etc. The same is true for the DrugIssue files. The naming structure for these is as follows:

- aurum_allpatid_set*X*_extract_patient_001.txt
- aurum_allpatid_set*X*_extract_observation_00*Y*.txt
- aurum_allpatid_set*X*_extract_drugissue_00*Y*.txt

where *X* and *Y* take integer values. We have provided simulated .txt patient, Observation and DrugIssue files with these naming conventions. This contains data on 6 fake patients from set 1, split across three observation and DrugIssue files. Note the "allpatid" in the filenames denotes that we are working with the entire CPRD AURUM database. This part of the filename may be altered if you have only been provided with data on a subset of individuals, however we expect the set *X* and suffix *Y* naming convention to be the same. We will showcase two approaches to extracting data for a cohort, one which is reliant on the data structure and naming conventions to reduce computational time, and one which has no reliance on these things.

