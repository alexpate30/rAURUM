@article{Gulliford2009,
abstract = {BACKGROUND: Electronic patient records from primary care databases are increasingly used in public health and health services research but methods used to identify cases with disease are not well described. This study aimed to evaluate the relevance of different codes for the identification of acute stroke in a primary care database, and to evaluate trends in the use of different codes over time. METHODS: Data were obtained from the General Practice Research Database from 1997 to 2006. All subjects had a minimum of 24 months of up-to-standard record before the first recorded stroke diagnosis. Initially, we identified stroke cases using a supplemented version of the set of codes for prevalent stroke used by the Office for National Statistics in Key health statistics from general practice 1998 (ONS codes). The ONS codes were then independently reviewed by four raters and a restricted set of 121 codes for 'acute stroke' was identified but the kappa statistic was low at 0.23. RESULTS: Initial extraction of data using the ONS codes gave 48,239 cases of stroke from 1997 to 2006. Application of the restricted set of codes reduced this to 39,424 cases. There were 2,288 cases whose index medical codes were for 'stroke annual review' and 3,112 for 'stroke monitoring'. The frequency of stroke review and monitoring codes as index codes increased from 9 per year in 1997 to 1,612 in 2004, 1,530 in 2005 and 1,424 in 2006. The one year mortality of cases with the restricted set of codes was 29.1{%} but for 'stroke annual review,' 4.6{%} and for 'stroke monitoring codes', 5.7{%}. CONCLUSION: In the analysis of electronic patient records, different medical codes for a single condition may have varying clinical and prognostic significance; utilisation of different medical codes may change over time; researchers with differing clinical or epidemiological experience may have differing interpretations of the relevance of particular codes. There is a need for greater transparency in the selection of sets of codes for different conditions, for the reporting of sensitivity analyses using different sets of codes, as well as sharing of code sets among researchers.},
author = {Gulliford, Martin C. and Charlton, Judith and Ashworth, Mark and Rudd, Anthony G. and Toschke, Andre Michael and Delaney, Brendan and Grieve, Andy and Heuschmann, Peter U. and Little, Paul and Redfern, Judith and van Staa, Tjeerd and Wolfe, Charles and Yardley, Lucy and McDermott, Lisa},
doi = {10.1371/journal.pone.0007168},
file = {:C\:/Users/mbrxsap3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gulliford et al. - 2009 - Selection of medical diagnostic codes for analysis of electronic patient records. Application to stroke in a p.pdf:pdf},
isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pmid = {19777060},
title = {{Selection of medical diagnostic codes for analysis of electronic patient records. Application to stroke in a primary care database}},
volume = {4},
year = {2009}
}
@misc{ClinicalPracticeResearchDatalink,
author = {CPRD},
title = {{CPRD GOLD June 2024 Dataset}},
url = {https://www.cprd.com/doi/cprd-gold-june-2024-dataset},
urldate = {2024-06-12},
year = {2024}
}
@article{Springate2017,
abstract = {Research with structured Electronic Health Records (EHRs) is expanding as data becomes more accessible; analytic methods advance; and the scientific validity of such studies is increasingly accepted. However, data science methodology to enable the rapid searching/ extraction, cleaning and analysis of these large, often complex, datasets is less well developed. In addition, commonly used software is inadequate, resulting in bottlenecks in research workflows and in obstacles to increased transparency and reproducibility of the research. Preparing a research-ready dataset from EHRs is a complex and time consuming task requiring substantial data science skills, even for simple designs. In addition, certain aspects of the workflow are computationally intensive, for example extraction of longitudinal data and matching controls to a large cohort, which may take days or even weeks to run using standard software. The rEHR package simplifies and accelerates the process of extracting ready-for-analysis datasets from EHR databases. It has a simple import function to a database backend that greatly accelerates data access times. A set of generic query functions allow users to extract data efficiently without needing detailed knowledge of SQL queries. Longitudinal data extractions can also be made in a single command, making use of parallel processing. The package also contains functions for cutting data by time-varying covariates, matching controls to cases, unit conversion and construction of clinical code lists. There are also functions to synthesise dummy EHR. The package has been tested with one for the largest primary care EHRs, the Clinical Practice Research Datalink (CPRD), but allows for a common interface to other EHRs. This simplified and accelerated work flow for EHR data extraction results in simpler, cleaner scripts that are more easily debugged, shared and reproduced.},
author = {Springate, David A. and Parisi, Rosa and Olier, Ivan and Reeves, David and Kontopantelis, Evangelos},
doi = {10.1371/journal.pone.0171784},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Springate2017.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--25},
pmid = {28231289},
title = {{rEHR: An R package for manipulating and analysing electronic health record data}},
volume = {12},
year = {2017}
}
@misc{ClinicalPracticeResearchDatalink2022,
author = {CPRD},
title = {{CPRD Aurum Data Specification. Version 2.7}},
url = {https://www.cprd.com/},
year = {2022}
}
@misc{Muller2024,
author = {M{\"{u}}ller, Kirill and Wickham, Hadley and James, David A. and Falcon, Seth},
title = {{RSQLite: SQLite Interface for R}},
url = {https://rsqlite.r-dbi.org},
year = {2024}
}
@article{Williams2019,
abstract = {Objective Clinical code sets are vital to research using routinely-collected electronic healthcare data. Existing code set engineering methods pose significant limitations when considering reproducible research. To improve the transparency and reusability of research, these code sets must abide by FAIR principles; this is not currently happening. We propose ‘term sets', an equivalent alternative to code sets that are findable, accessible, interoperable and reusable. Materials and methods We describe a new code set representation, consisting of natural language inclusion and exclusion terms (term sets), and explain its relationship to code sets. We formally prove that any code set has a corresponding term set. We demonstrate utility by searching for recently published code sets, representing them as term sets, and reporting on the number of inclusion and exclusion terms compared with the size of the code set. Results Thirty-one code sets from 20 papers covering diverse disease domains were converted into term sets. The term sets were on average 74% the size of their equivalent original code set. Four term sets were larger due to deficiencies in the original code sets. Discussion Term sets can concisely represent any code set. This may reduce barriers for examining and reusing code sets, which may accelerate research using healthcare databases. We have developed open-source software that supports researchers using term sets. Conclusion Term sets are independent of clinical code terminologies and therefore: enable reproducible research; are resistant to terminology changes; and are less error-prone as they are shorter than the equivalent code set.},
author = {Williams, Richard and Brown, Benjamin and Kontopantelis, Evan and van Staa, Tjeerd and Peek, Niels},
doi = {10.1371/journal.pone.0212291},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Williams2019.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--15},
pmid = {30763407},
title = {{Term sets: A transparent and reproducible representation of clinical code sets}},
volume = {14},
year = {2019}
}
@article{Watson2017,
abstract = {Objective Analysis of routinely collected electronic health record (EHR) data from primary care is reliant on the creation of codelists to define clinical features of interest. To improve scientific rigour, transparency and replicability, we describe and demonstrate a standardised reproducible methodology for clinical codelist development. Design We describe a three-stage process for developing clinical codelists. First, the clear definition a priori of the clinical feature of interest using reliable clinical resources. Second, development of a list of potential codes using statistical software to comprehensively search all available codes. Third, a modified Delphi process to reach consensus between primary care practitioners on the most relevant codes, including the generation of an 'uncertainty' variable to allow sensitivity analysis. Setting These methods are illustrated by developing a codelist for shortness of breath in a primary care EHR sample, including modifiable syntax for commonly used statistical software. Participants The codelist was used to estimate the frequency of shortness of breath in a cohort of 28 216 patients aged over 18 years who received an incident diagnosis of lung cancer between 1 January 2000 and 30 November 2016 in the Clinical Practice Research Datalink (CPRD). Results Of 78 candidate codes, 29 were excluded as inappropriate. Complete agreement was reached for 44 (90%) of the remaining codes, with partial disagreement over 5 (10%). 13 091 episodes of shortness of breath were identified in the cohort of 28 216 patients. Sensitivity analysis demonstrates that codes with the greatest uncertainty tend to be rarely used in clinical practice. Conclusions Although initially time consuming, using a rigorous and reproducible method for codelist generation 'future-proofs' findings and an auditable, modifiable syntax for codelist generation enables sharing and replication of EHR studies. Published codelists should be badged by quality and report the methods of codelist generation including: definitions and justifications associated with each codelist; the syntax or search method; the number of candidate codes identified; and the categorisation of codes after Delphi review.},
author = {Watson, Jessica and Nicholson, Brian D. and Hamilton, Willie and Price, Sarah},
doi = {10.1136/bmjopen-2017-019637},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Watson2017.pdf:pdf},
issn = {20446055},
journal = {BMJ Open},
keywords = {Clinical coding,Electronic health records,Epidemiology,Primary care},
number = {11},
pages = {1--9},
pmid = {29170293},
title = {{Identifying clinical features in primary care electronic health record studies: Methods for codelist development}},
volume = {7},
year = {2017}
}
@article{Williams2017,
abstract = {Introduction The construction of reliable, reusable clinical code sets is essential when re-using Electronic Health Record (EHR) data for research. Yet code set definitions are rarely transparent and their sharing is almost non-existent. There is a lack of methodological standards for the management (construction, sharing, revision and reuse) of clinical code sets which needs to be addressed to ensure the reliability and credibility of studies which use code sets. Objective To review methodological literature on the management of sets of clinical codes used in research on clinical databases and to provide a list of best practice recommendations for future studies and software tools. Methods We performed an exhaustive search for methodological papers about clinical code set engineering for re-using EHR data in research. This was supplemented with papers identified by snowball sampling. In addition, a list of e-phenotyping systems was constructed by merging references from several systematic reviews on this topic, and the processes adopted by those systems for code set management was reviewed. Results Thirty methodological papers were reviewed. Common approaches included: creating an initial list of synonyms for the condition of interest (n = 20); making use of the hierarchical nature of coding terminologies during searching (n = 23); reviewing sets with clinician input (n = 20); and reusing and updating an existing code set (n = 20). Several open source software tools (n = 3) were discovered. Discussion There is a need for software tools that enable users to easily and quickly create, revise, extend, review and share code sets and we provide a list of recommendations for their design and implementation. Conclusion Research re-using EHR data could be improved through the further development, more widespread use and routine reporting of the methods by which clinical codes were selected.},
author = {Williams, Richard and Kontopantelis, Evangelos and Buchan, Iain and Peek, Niels},
doi = {10.1016/j.jbi.2017.04.010},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Williams2017.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Clinical codes,Code list,Code set,Phenotyping,Review,Value set},
pages = {1--13},
pmid = {28442434},
publisher = {The Author(s)},
title = {{Clinical code set engineering for reusing EHR data for research: A review}},
url = {http://dx.doi.org/10.1016/j.jbi.2017.04.010},
volume = {70},
year = {2017}
}
@misc{CPRD2024,
author = {CPRD},
title = {{CPRD Aurum March 2024 Dataset}},
url = {https://www.cprd.com/doi/cprd-aurum-march-2024-dataset},
urldate = {12/06/2024},
year = {2024}
}
@misc{TheHealthFoundationAnalyticsLab2021,
author = {{The Health Foundation Analytics Lab}},
title = {aurumpipeline},
url = {https://github.com/HFAnalyticsLab/aurumpipeline},
year = {2021}
}
@misc{Yimer2021,
author = {Yimer, Belay Birlie and Selby, David and Jani, Meghna and Nenadic, Goran and Lunt, Mark and Dixon, William G.},
title = {{drugprepr: Prepare Electronic Prescription Record Data to Estimate Drug Exposure}},
url = {https://cran.r-project.org/package=drugprepr},
year = {2021}
}
@article{Pye2018,
abstract = {Purpose: Real‐world data for observational research commonly require formatting and cleaning prior to analysis. Data preparation steps are rarely reported adequately and are likely to vary between research groups. Variation in methodology could potentially affect study outcomes. This study aimed to develop a framework to define and document drug data preparation and to examine the impact of different assumptions on results. Methods: An algorithm for processing prescription data was developed and tested using data from the Clinical Practice Research Datalink (CPRD). The impact of varying assumptions was examined by estimating the association between 2 exemplar medications (oral hypoglycaemic drugs and glucocorticoids) and cardiovascular events after preparing multiple datasets derived from the same source prescription data. Each dataset was analysed using Cox proportional hazards modelling. Results: The algorithm included 10 decision nodes and 54 possible unique assump-tions. Over 11 000 possible pathways through the algorithm were identified. In both exemplar studies, similar hazard ratios and standard errors were found for the majority of pathways; however, certain assumptions had a greater influence on results. For example, in the hypoglycaemic analysis, choosing a different variable to define prescription end date altered the hazard ratios (95% confidence intervals) from 1.77 (1.56‐2.00) to 2.83 (1.59‐5.04). Conclusions: The framework offers a transparent and efficient way to perform and report drug data preparation steps. Assumptions made during data preparation can impact the results of analyses. Improving transparency regarding drug data preparation would increase the repeatability, reproducibility, and comparability of published results.},
author = {Pye, Stephen R. and Sheppard, Th{\'{e}}r{\`{e}}se and Joseph, Rebecca M. and Lunt, Mark and Girard, Nadyne and Haas, Jennifer S. and Bates, David W. and Buckeridge, David L. and van Staa, Tjeerd P. and Tamblyn, Robyn and Dixon, William G.},
doi = {10.1002/pds.4440},
file = {:C\:/DownloadsC/Pye2017v1.pdf:pdf},
issn = {10991557},
journal = {Pharmacoepidemiology and Drug Safety},
keywords = {Data preparation,Pharmacoepidemiology,Reproducibility,Transparency},
number = {7},
pages = {781--788},
pmid = {29667263},
title = {{Assumptions made when preparing drug exposure data for analysis have an impact on results: An unreported step in pharmacoepidemiology studies}},
volume = {27},
year = {2018}
}
